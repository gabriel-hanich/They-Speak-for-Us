{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a4e3e55",
   "metadata": {},
   "source": [
    "# Analyse server stored data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2734784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import time\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.style\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import operator\n",
    "\n",
    "from src.media import Outlet, Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e402d4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions\n",
    "def getDateRange(start_date, end_date):  # Return list of datetime.date objects between start_date and end_date (inclusive).\n",
    "    date_list = []\n",
    "    curr_date = start_date\n",
    "    while curr_date <= end_date:\n",
    "        date_list.append(curr_date)\n",
    "        curr_date += timedelta(days=1)\n",
    "    return date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6644714f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make maptlotlib show graphs in new window\n",
    "%matplotlib qt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2968257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "# Date range for articles being scraped from the server\n",
    "startScrapeDate = \"01/10/2022\"\n",
    "endScrapeDate = \"14/11/2022\"\n",
    "\n",
    "collectionCap = -1 # The maximum amount of articles to get pulled from the server (set to -1 for uncaped scraping)\n",
    "\n",
    "startScrapeDate = datetime.strptime(startScrapeDate, \"%d/%m/%Y\")\n",
    "endScrapeDate = datetime.strptime(endScrapeDate, \"%d/%m/%Y\")\n",
    "stopwordsSet = set(stopwords.words('english'))\n",
    "exclusionList = [\"say\", \"new\", \"news\", \"day\", \"days\"]\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c58a7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load setup data\n",
    "with open(\"./settings.json\", \"r\") as setupFile:\n",
    "    setupData = json.load(setupFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8591f4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Collected article number 48843\n",
      " Collected a total of 48843 articles in 18.754 seconds\n"
     ]
    }
   ],
   "source": [
    "articleList = []\n",
    "startScanTime = time.time() # Track the time elapsed \n",
    "DBClient = MongoClient(setupData[\"DB_URI\"], server_api = ServerApi('1')) # Connect to the database\n",
    "\n",
    "# Get articles from DBClient\n",
    "articleCollection = DBClient[setupData[\"DB_NAME\"]]['newsData']\n",
    "articleCursor = articleCollection.aggregate([{'$match': {'publishDate': {\n",
    "                '$gt': startScrapeDate, \n",
    "                '$lt': endScrapeDate\n",
    "        }}}])\n",
    "\n",
    "for articleIndex, article in enumerate(articleCursor):\n",
    "    articleList.append(Article(\n",
    "                article[\"outletName\"],\n",
    "                article[\"headline\"],\n",
    "                article[\"description\"],\n",
    "                article[\"author\"],\n",
    "                article[\"publishDate\"],\n",
    "                article[\"sentimentScore\"]\n",
    "        ))\n",
    "    \n",
    "    print(f\"\\r Collected article number {articleIndex + 1}\", end=\"\")\n",
    "print(f\"\\n Collected a total of {len(articleList)} articles in {round((time.time() - startScanTime), 3)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1deb0684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26 outlets in total\n"
     ]
    }
   ],
   "source": [
    "# Sort articles by outlet\n",
    "outletList = []\n",
    "\n",
    "for articleIndex, article in enumerate(articleList):\n",
    "    foundOutlet = False # If the outlet has been found within `outletList`\n",
    "    for outlet in outletList:\n",
    "        if outlet.name == article.outlet:\n",
    "            outlet.addArticle(article)\n",
    "            foundOutlet = True\n",
    "            break\n",
    "    if not foundOutlet: # Make new outlet\n",
    "        newOutlet = Outlet(article.outlet)\n",
    "        outletList.append(newOutlet)    \n",
    "print(f\"Found {len(outletList)} outlets in total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45e1f138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Daily Mail ===\n",
      "Published a total of 9171 articles\n",
      "Has an average sentiment of -0.207\n",
      "\n",
      "\n",
      "=== The Guardian ===\n",
      "Published a total of 5195 articles\n",
      "Has an average sentiment of -0.073\n",
      "\n",
      "\n",
      "=== CNN ===\n",
      "Published a total of 3058 articles\n",
      "Has an average sentiment of -0.098\n",
      "\n",
      "\n",
      "=== BuzzFeed ===\n",
      "Published a total of 2917 articles\n",
      "Has an average sentiment of 0.065\n",
      "\n",
      "\n",
      "=== South China Morning Post ===\n",
      "Published a total of 2797 articles\n",
      "Has an average sentiment of -0.071\n",
      "\n",
      "\n",
      "=== ABC News ===\n",
      "Published a total of 2466 articles\n",
      "Has an average sentiment of -0.115\n",
      "\n",
      "\n",
      "=== Huffington Post ===\n",
      "Published a total of 2435 articles\n",
      "Has an average sentiment of -0.069\n",
      "\n",
      "\n",
      "=== CBS News ===\n",
      "Published a total of 2372 articles\n",
      "Has an average sentiment of -0.07\n",
      "\n",
      "\n",
      "=== Fox News ===\n",
      "Published a total of 2111 articles\n",
      "Has an average sentiment of -0.135\n",
      "\n",
      "\n",
      "=== Sydney Morning Herald ===\n",
      "Published a total of 2102 articles\n",
      "Has an average sentiment of -0.064\n",
      "\n",
      "\n",
      "=== The Age ===\n",
      "Published a total of 2073 articles\n",
      "Has an average sentiment of -0.061\n",
      "\n",
      "\n",
      "=== Alja Zeera ===\n",
      "Published a total of 1761 articles\n",
      "Has an average sentiment of -0.131\n",
      "\n",
      "\n",
      "=== USA Today ===\n",
      "Published a total of 1584 articles\n",
      "Has an average sentiment of -0.076\n",
      "\n",
      "\n",
      "=== New York Times ===\n",
      "Published a total of 1160 articles\n",
      "Has an average sentiment of -0.127\n",
      "\n",
      "\n",
      "=== Amercian ABC ===\n",
      "Published a total of 1159 articles\n",
      "Has an average sentiment of -0.213\n",
      "\n",
      "\n",
      "=== BBC News ===\n",
      "Published a total of 1143 articles\n",
      "Has an average sentiment of -0.207\n",
      "\n",
      "\n",
      "=== 9 News ===\n",
      "Published a total of 1112 articles\n",
      "Has an average sentiment of -0.175\n",
      "\n",
      "\n",
      "=== CNBC ===\n",
      "Published a total of 812 articles\n",
      "Has an average sentiment of -0.027\n",
      "\n",
      "\n",
      "=== Daily Telegraph ===\n",
      "Published a total of 674 articles\n",
      "Has an average sentiment of -0.07\n",
      "\n",
      "\n",
      "=== Michael West ===\n",
      "Published a total of 619 articles\n",
      "Has an average sentiment of -0.032\n",
      "\n",
      "\n",
      "=== Sky News ===\n",
      "Published a total of 549 articles\n",
      "Has an average sentiment of -0.222\n",
      "\n",
      "\n",
      "=== The Conversation ===\n",
      "Published a total of 481 articles\n",
      "Has an average sentiment of -0.043\n",
      "\n",
      "\n",
      "=== Wall Street Journal ===\n",
      "Published a total of 344 articles\n",
      "Has an average sentiment of -0.109\n",
      "\n",
      "\n",
      "=== Crikey ===\n",
      "Published a total of 337 articles\n",
      "Has an average sentiment of -0.069\n",
      "\n",
      "\n",
      "=== The Washington Post ===\n",
      "Published a total of 244 articles\n",
      "Has an average sentiment of -0.164\n",
      "\n",
      "\n",
      "=== Independent Australia ===\n",
      "Published a total of 141 articles\n",
      "Has an average sentiment of -0.075\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outletList.sort(key = lambda x : len(x.articleList), reverse=True)\n",
    "# Text outputs for each outlet\n",
    "for outlet in outletList:\n",
    "    # Get the average sentiment\n",
    "    avgSentiment = sum(list(article.sentimentScore for article in outlet.articleList)) / len(outlet.articleList)\n",
    "    avgSentiment = avgSentiment \n",
    "        \n",
    "    print(f\"{'=' * 3} {outlet.name} {'=' * 3}\")\n",
    "    print(f\"Published a total of {len(outlet.articleList)} articles\")\n",
    "    print(f\"Has an average sentiment of {round(avgSentiment, 3)}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b796a9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 most prolific journalists are:\n",
      "- None - 27156 | ['BuzzFeed', 'CNN', 'CBS News', 'ABC News', 'Fox News', 'Huffington Post', 'BBC News', 'Amercian ABC', 'Alja Zeera', 'Daily Mail', 'Wall Street Journal', 'CNBC', 'Sky News', 'The Conversation', 'New York Times']\n",
      "-  - 2756 | ['Daily Telegraph', 'The Age', 'Sydney Morning Herald', 'The Guardian', 'South China Morning Post', 'Independent Australia']\n",
      "- 9News - 1113 | ['9 News']\n",
      "- AAP - 555 | ['Michael West']\n",
      "- Associated Press - 328 | ['South China Morning Post', 'The Guardian', 'USA Today', 'The Washington Post']\n",
      "- Reuters - 308 | ['South China Morning Post', 'The Guardian', 'Sydney Morning Herald', 'The Age', 'New York Times']\n",
      "- Agence France-Presse - 293 | ['South China Morning Post', 'The Guardian']\n",
      "- Australian Associated Press - 214 | ['The Guardian']\n",
      "- Bloomberg - 143 | ['South China Morning Post']\n",
      "- The New York Times - 109 | ['New York Times']\n"
     ]
    }
   ],
   "source": [
    "# Text outputs for each journalist\n",
    "journalistList = []\n",
    "journalistOutput = {}\n",
    "for article in articleList:\n",
    "    journalistList.append(article.author)\n",
    "    try:\n",
    "        if article.outlet not in journalistOutput[article.author]:\n",
    "           journalistOutput[article.author].append(article.outlet)\n",
    "        \n",
    "    except KeyError:\n",
    "        journalistOutput[article.author] = [article.outlet]\n",
    "\n",
    "print(f\"The 10 most prolific journalists are:\")\n",
    "for journalist in Counter(journalistList).most_common(10):\n",
    "    print(f\"- {journalist[0]} - {journalist[1]} | {journalistOutput[journalist[0]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a25483a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display parameters\n",
    "topicList = [[\"democrats\", \"joe\", \"biden\", \"democrat\", \"blue\"], [\"republicans\", \"red\", \"trump\", \"donald\",\"republican\"]] # Which topics to use (leave blank for all) (MUST BE LOWERCASE)\n",
    "showOutletsList = [] # Which outlets to be shown (leave blank for all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2e9d031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect data to plot\n",
    "plotArticles = {} # Stores the articles by the topic \n",
    "if topicList == []:\n",
    "    plotArticles['total'] = [] # If there are no topic, total is used to show the total amt of articles\n",
    "    for article in articleList:\n",
    "        if article.outlet in showOutletsList or showOutletsList == []:\n",
    "            plotArticles['total'].append(article)\n",
    "else:            \n",
    "    for topic in topicList: \n",
    "        plotArticles[topic[0]] = [] # Each topic stores corresponding articles in a list\n",
    "        for article in articleList:\n",
    "            if article.outlet in showOutletsList or showOutletsList == []: # If the article is from the specified outlet\n",
    "                for word in article.headline.split(\" \"):\n",
    "                    if word.lower() in topic: # If a given word from the article is in the topic searchlist\n",
    "                        plotArticles[topic[0]].append(article)\n",
    "                        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2daff92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average sentimentScore for democrats was -0.07924705111312151\n",
      "The average sentimentScore for republicans was -0.06850028889152535\n"
     ]
    }
   ],
   "source": [
    "# Plot daily average (or total output) for any attribute over time, as a total/avg of all outlets\n",
    "plotAttribute = \"sentimentScore\" # The article attribute to be avg'd and plotted over time (set to `publishCount` for daily TOTAL output)\n",
    "plotDates = getDateRange(startScrapeDate, endScrapeDate)\n",
    "\n",
    "for topic in plotArticles.keys():\n",
    "    plotData = {}\n",
    "    for dateIndex, date in enumerate(plotDates):\n",
    "        if dateIndex + 1 != len(plotDates):\n",
    "            plotData[date] = []\n",
    "    for article in plotArticles[topic]:\n",
    "        articleDate = article.date\n",
    "        if plotAttribute == \"publishCount\":\n",
    "            plotData[articleDate.replace(hour=0, minute=0, second=0)].append(1)\n",
    "        else:\n",
    "            plotData[articleDate.replace(hour=0, minute=0, second=0)].append(getattr(article, plotAttribute))\n",
    "    \n",
    "    # Plot the data\n",
    "    xVals = list(plotData.keys())\n",
    "    yVals = []\n",
    "    for val in xVals:\n",
    "        try:\n",
    "            if plotAttribute == \"publishCount\":\n",
    "                yVals.append(len(plotData[val]))\n",
    "            else:\n",
    "                yVals.append(sum(plotData[val]) / len(plotData[val]))\n",
    "        except ZeroDivisionError:\n",
    "            yVals.append(0)\n",
    "    if plotAttribute == \"publishCount\":\n",
    "        print(f\"In total, {sum(yVals)} articles got published about {topic}\")\n",
    "    else:\n",
    "        print(f\"The average {plotAttribute} for {topic} was {sum(yVals) / len(yVals)}\")\n",
    "    plt.plot(xVals, yVals, label=topic)\n",
    "    \n",
    "plt.title(f\"{plotAttribute} over time\")\n",
    "plt.legend()\n",
    "# plt.ylim((-1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1ee6dc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For ABC News the overall average for putin was -0.06\n",
      "For 9 News the overall average for putin was -0.0323\n",
      "For Sydney Morning Herald the overall average for putin was -0.0545\n",
      "For SBS Australia the overall average for putin was -0.0017\n",
      "For Independent Australia the overall average for putin was -0.004\n",
      "For Daily Telegraph the overall average for putin was -0.0048\n",
      "For The Age the overall average for putin was -0.0502\n",
      "For Michael West the overall average for putin was -0.0028\n",
      "For The Guardian the overall average for putin was -0.1379\n",
      "For Crikey the overall average for putin was -0.0046\n",
      "For CNN the overall average for putin was -0.0547\n",
      "For New York Times the overall average for putin was -0.0583\n",
      "For Fox News the overall average for putin was -0.047\n",
      "For BBC News the overall average for putin was -0.0985\n",
      "For The Conversation the overall average for putin was -0.0124\n",
      "For The Washington Post the overall average for putin was -0.0433\n",
      "For Wall Street Journal the overall average for putin was -0.0394\n",
      "For CNBC the overall average for putin was -0.0477\n",
      "For Daily Mail the overall average for putin was -0.1696\n",
      "For Huffington Post the overall average for putin was -0.0296\n",
      "For BuzzFeed the overall average for putin was 0.0\n",
      "For Amercian ABC the overall average for putin was -0.0202\n",
      "For The Economist the overall average for putin was 0.0\n",
      "For Sky News the overall average for putin was -0.0662\n",
      "For CBS News the overall average for putin was -0.0726\n",
      "For Alja Zeera the overall average for putin was -0.0486\n",
      "For USA Today the overall average for putin was -0.0442\n",
      "For South China Morning Post the overall average for putin was -0.1475\n"
     ]
    }
   ],
   "source": [
    "# Plot any attribute over time but broken down by outlet\n",
    "plotAttribute = \"sentimentScore\" # The article attribute to be avg'd and plotted over time (set to `publishCount` for daily TOTAL output)\n",
    "plotDates = getDateRange(startScrapeDate, endScrapeDate) # A list of all the dates that will be plotted\n",
    "\n",
    "displayOutlets = [] # The list that keeps track of which outlets to display\n",
    "if showOutletsList == []: # If the user has not specified which outlets to show, show all of them\n",
    "    for outlet in outletList:\n",
    "        displayOutlets.append(outlet.name) \n",
    "else:\n",
    "    displayOutlets = showOutletsList # Else show only those specified\n",
    "\n",
    "for topic in plotArticles.keys():\n",
    "    for outlet in displayOutlets: # Show how each media outlet reports each topic\n",
    "        plotData = {} # Dict containing each display date as key, and the list of scores for that day as value\n",
    "        for dateIndex, date in enumerate(plotDates):\n",
    "            if dateIndex + 1 != len(plotDates):\n",
    "                plotData[date] = []\n",
    "                \n",
    "        for article in plotArticles[topic]:\n",
    "            if article.outlet == outlet:\n",
    "                articleDate = article.date\n",
    "                if plotAttribute == \"publishCount\": # If the user is trying to find how many articles have been published on a given day, add 1 per article\n",
    "                    plotData[articleDate.replace(hour=0, minute=0, second=0)].append(1)\n",
    "                else:\n",
    "                    plotData[articleDate.replace(hour=0, minute=0, second=0)].append(getattr(article, plotAttribute)) # Append the score to the daily list\n",
    "        \n",
    "        # Plot the data\n",
    "        xVals = list(plotData.keys())\n",
    "        yVals = []\n",
    "        for val in xVals:\n",
    "            try:\n",
    "                if plotAttribute == \"publishCount\":\n",
    "                    yVals.append(len(plotData[val])) # Plot the daily count (total)\n",
    "                else:\n",
    "                    yVals.append(sum(plotData[val]) / len(plotData[val])) # plot the daily average \n",
    "            except ZeroDivisionError:\n",
    "                yVals.append(0) # If there are no datapoints for the day, display 0\n",
    "        plt.plot(xVals, yVals, label=f\"{outlet} - {topic}\")\n",
    "        \n",
    "        if plotAttribute == \"publishCount\":\n",
    "            print(f\"In total, {outlet} published {sum(yVals)} articles about {topic}\")\n",
    "        else:\n",
    "            print(f\"For {outlet} the overall average for {topic} was {round(sum(yVals) / len(yVals), 4)}\")\n",
    "plt.title(f\"{plotAttribute} Over time by Outlet\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd2cbb24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common keywords for republicans were\n",
      "- trump - 44\n",
      "- republican - 35\n",
      "- red - 17\n",
      "- midterm - 8\n",
      "- donald - 7\n",
      "- say - 4\n",
      "- rally - 3\n",
      "- biden - 3\n",
      "- election - 3\n",
      "- asks - 2\n",
      "- subpoena - 2\n",
      "- jan. - 2\n",
      "- 2024 - 2\n",
      "- twitter - 2\n",
      "- tax - 2\n"
     ]
    }
   ],
   "source": [
    "# Find keywords for each day by topic\n",
    "dailyDisplay = 4 # The number of keywords that gets displayed for each date\n",
    "displayTopic = \"republicans\" # The topic that gets graphed\n",
    "minTextScore = 2 # The minimum number of a times a keyword needs to be mentioned in order to get it's text displayed\n",
    "\n",
    "\n",
    "try:\n",
    "    plotArticles[displayTopic]\n",
    "except KeyError:\n",
    "    print(f\"Topic {displayTopic} is not available, the possible topics are {list(plotArticles.keys())}\")\n",
    "    \n",
    "\n",
    "keywordColors = {} # Dict containing the color for each keyword\n",
    "plotDates = getDateRange(startScrapeDate, endScrapeDate) # A list of all the dates that will be plotted\n",
    "totalKeywords = [] # All the keywords and their freqency\n",
    "\n",
    "datedKeywords = {} # A dict containg all the keywords in articles from a given date about the topic\n",
    "for dateIndex, date in enumerate(plotDates):\n",
    "    if dateIndex + 1 != len(plotDates):\n",
    "        datedKeywords[date] = []\n",
    "\n",
    "for article in plotArticles[displayTopic]:\n",
    "    if article.outlet in showOutletsList or showOutletsList == []: # If the article is from the specified outlet\n",
    "        articleDate = article.date\n",
    "        for word in article.headline.split(\" \"):\n",
    "            word = word.strip().lower()\n",
    "            if word not in stopwordsSet and len(word) > 2 and word not in exclusionList:\n",
    "                datedKeywords[articleDate.replace(hour=0, minute=0, second=0)].append(lemmatizer.lemmatize(word)) # Append the (lemmatized) word to the dict for the given date\n",
    "lastKeywords = []\n",
    "for date in datedKeywords.keys():\n",
    "    keywords = Counter(datedKeywords[date]).most_common(dailyDisplay)\n",
    "    for keyword in keywords:\n",
    "        try:\n",
    "            keywordColor = keywordColors[keyword[0]] # If the keyword already has a color for itself\n",
    "        except KeyError:\n",
    "            r = lambda: random.randint(0,255) # Else, generate a new color for the keyword\n",
    "            keywordColor = '#%02X%02X%02X' % (r(),r(),r())\n",
    "            keywordColors[keyword[0]] = keywordColor # If this is the first time \n",
    "\n",
    "        totalKeywords.append(keyword[0])\n",
    "        plt.scatter(date, keyword[1], color=keywordColor, label=keyword[0]) # Put the point on the graph\n",
    "\n",
    "        # Draw lines between points with the same keyword\n",
    "        foundPrior = False # Tracks whether the date before contains the same keyword\n",
    "        for lastKeyword in lastKeywords:\n",
    "            if lastKeyword[0] == keyword[0]:\n",
    "                plt.plot([lastDate, date], [lastKeyword[1], keyword[1]], color=keywordColor)\n",
    "                foundPrior = True\n",
    "                break\n",
    "\n",
    "        if not foundPrior and keyword[1] >= minTextScore: # Only display text if the point is at the start of a 'chain'\n",
    "            plt.text(date, keyword[1], keyword[0])\n",
    "\n",
    "    # Save the last date and keywords to plot lines in the next date\n",
    "    lastDate = date \n",
    "    lastKeywords = keywords\n",
    "\n",
    "print(f\"The most common keywords for {displayTopic} were\")\n",
    "for keyword, keyFreq in Counter(totalKeywords).most_common(15):\n",
    "    print(f\"- {keyword} - {keyFreq}\")\n",
    "    \n",
    "plt.title(f\"Keywords over time for topic {displayTopic}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28e06e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram for the number of articles with each sentiment scores\n",
    "displayTopic = \"republicans\" # The topic that gets graphed\n",
    "incrementCount = 50\n",
    "\n",
    "try:\n",
    "    plotArticles[displayTopic]\n",
    "except KeyError:\n",
    "    print(f\"Topic {displayTopic} is not available, the possible topics are {list(plotArticles.keys())}\")\n",
    "    \n",
    "sentimentData = []\n",
    "for article in plotArticles[displayTopic]:\n",
    "    sentimentData.append(article.sentimentScore)\n",
    "    \n",
    "plt.hist(sentimentData, incrementCount)\n",
    "plt.title(f\"Number of articles with each sentiment for topic {displayTopic}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fb3a39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
