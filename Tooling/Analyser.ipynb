{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse server stored data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import time\n",
    "from collections import Counter\n",
    "import random\n",
    "import csv\n",
    "\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.style\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import operator\n",
    "\n",
    "from src.media import Outlet, Article\n",
    "from src.data import getNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions\n",
    "def getDateRange(start_date, end_date):  # Return list of datetime.date objects between start_date and end_date (inclusive).\n",
    "    date_list = []\n",
    "    curr_date = start_date\n",
    "    while curr_date <= end_date:\n",
    "        date_list.append(curr_date)\n",
    "        curr_date += timedelta(days=1)\n",
    "    return date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make maptlotlib show graphs in new window\n",
    "%matplotlib qt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "# Date range for articles being scraped from the server\n",
    "startScrapeDate = \"01/03/2023\"\n",
    "endScrapeDate = \"30/04/2023\"\n",
    "\n",
    "collectionCap = -1 # The maximum amount of articles to get pulled from the server (set to -1 for uncaped scraping)\n",
    "\n",
    "startScrapeDate = datetime.strptime(startScrapeDate, \"%d/%m/%Y\")\n",
    "endScrapeDate = datetime.strptime(endScrapeDate, \"%d/%m/%Y\")\n",
    "stopwordsSet = set(stopwords.words('english'))\n",
    "exclusionList = [\"say\", \"new\", \"news\", \"day\", \"days\"]\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load setup data\n",
    "with open(\"./settings.json\", \"r\") as setupFile:\n",
    "    setupData = json.load(setupFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articleList = []\n",
    "startScanTime = time.time() # Track the time elapsed \n",
    "client = MongoClient(setupData[\"DB_URI\"], server_api=ServerApi('1'))\n",
    "# Get articles from DBClient\n",
    "articleCollection = client[setupData[\"DB_NAME\"]]['newsData']\n",
    "articleCursor = articleCollection.aggregate([{'$match': {'publishDate': {\n",
    "                '$gt': startScrapeDate, \n",
    "                '$lt': endScrapeDate\n",
    "        }}}])\n",
    "\n",
    "for articleIndex, article in enumerate(articleCursor):\n",
    "    if(article[\"isLegacy\"]):\n",
    "        articleList.append(Article(\n",
    "            article[\"outletName\"],\n",
    "            article[\"headline\"],\n",
    "            article[\"description\"],\n",
    "            article[\"author\"],\n",
    "            article[\"publishDate\"],\n",
    "            article[\"sentimentScore\"],\n",
    "            \"\",\n",
    "            []\n",
    "        ))\n",
    "    else:\n",
    "        articleList.append(Article(\n",
    "            article[\"outletName\"],\n",
    "            article[\"headline\"],\n",
    "            article[\"description\"],\n",
    "            article[\"author\"],\n",
    "            article[\"publishDate\"],\n",
    "            article[\"sentimentScore\"],\n",
    "            article[\"linkToArticle\"],\n",
    "            article[\"catergories\"]\n",
    "        ))\n",
    "    \n",
    "    print(f\"\\r Collected article number {articleIndex + 1}\", end=\"\")\n",
    "print(f\"\\n Collected a total of {len(articleList)} articles in {round((time.time() - startScanTime), 3)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort articles by outlet\n",
    "outletList = []\n",
    "\n",
    "for articleIndex, article in enumerate(articleList):\n",
    "    foundOutlet = False # If the outlet has been found within `outletList`\n",
    "    for outlet in outletList:\n",
    "        if outlet.name == article.outlet:\n",
    "            outlet.addArticle(article)\n",
    "            foundOutlet = True\n",
    "            break\n",
    "    if not foundOutlet: # Make new outlet\n",
    "        newOutlet = Outlet(article.outlet)\n",
    "        outletList.append(newOutlet)    \n",
    "print(f\"Found {len(outletList)} outlets in total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outletList.sort(key = lambda x : len(x.articleList), reverse=True)\n",
    "# Text outputs for each outlet\n",
    "for outlet in outletList:\n",
    "    # Get the average sentiment\n",
    "    avgSentiment = sum(list(article.sentimentScore for article in outlet.articleList)) / len(outlet.articleList)\n",
    "    avgSentiment = avgSentiment \n",
    "        \n",
    "    print(f\"{'=' * 3} {outlet.name} {'=' * 3}\")\n",
    "    print(f\"Published a total of {len(outlet.articleList)} articles\")\n",
    "    print(f\"Has an average sentiment of {round(avgSentiment, 3)}\")\n",
    "    print(\"\\n\")\n",
    "#     print(f\"{outlet.name},{len(outlet.articleList)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display parameters\n",
    "topicList = [[\"france\", \"french\",\"pension\",\"retirement\"], [\"aukus\", \"AUKUS\"]] # Which topics to use (leave blank for all) (MUST BE LOWERCASE)\n",
    "showOutletsList = [\"ABC News\", \"Al Arabiya\", \"Alja Zeera\", \"BBC News\", \"DW News\", \"Islamic Republic News Agency\", \"Russia Today\"] # Which outlets to be shown (leave blank for all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect data to plot\n",
    "plotArticles = {} # Stores the articles by the topic \n",
    "if len(topicList) == 0:\n",
    "    plotArticles['total'] = [] # Also include the total\n",
    "    for article in articleList:\n",
    "        if article.outlet in showOutletsList or showOutletsList == []:\n",
    "            plotArticles['total'].append(article)\n",
    "else:            \n",
    "    for topic in topicList: \n",
    "        plotArticles[topic[0]] = [] # Each topic stores corresponding articles in a list\n",
    "        for article in articleList:\n",
    "            if article.outlet in showOutletsList or showOutletsList == []: # If the article is from the specified outlet\n",
    "                for word in article.headline.split(\" \"):\n",
    "                    if word.lower() in topic or \"\" in topic: # If a given word from the article is in the topic searchlist\n",
    "                        plotArticles[topic[0]].append(article)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot daily average (or total output) for any attribute over time, as a total/avg of all outlets\n",
    "plotAttribute = \"publishCount\" # The article attribute to be avg'd and plotted over time (set to `publishCount` for daily TOTAL output)\n",
    "plotDates = getDateRange(startScrapeDate, endScrapeDate)\n",
    "\n",
    "for topic in plotArticles.keys():\n",
    "    plotData = {}\n",
    "    for dateIndex, date in enumerate(plotDates):\n",
    "        if dateIndex + 1 != len(plotDates):\n",
    "            plotData[date] = []\n",
    "    for article in plotArticles[topic]:\n",
    "        articleDate = article.date\n",
    "        try:\n",
    "            if plotAttribute == \"publishCount\":\n",
    "                plotData[articleDate.replace(hour=0, minute=0, second=0)].append(1)\n",
    "            else:\n",
    "                plotData[articleDate.replace(hour=0, minute=0, second=0)].append(getattr(article, plotAttribute))\n",
    "        except KeyError:\n",
    "            pass\n",
    "    # Plot the data\n",
    "    xVals = list(plotData.keys())\n",
    "    yVals = []\n",
    "    for val in xVals:\n",
    "        try:\n",
    "            if plotAttribute == \"publishCount\":\n",
    "                yVals.append(len(plotData[val]))\n",
    "            else:\n",
    "                yVals.append(sum(plotData[val]) / len(plotData[val]))\n",
    "        except ZeroDivisionError:\n",
    "            yVals.append(0)\n",
    "\n",
    "    if plotAttribute == \"publishCount\":\n",
    "        print(f\"In total, {sum(yVals)} articles got published about {topic}\")\n",
    "    else:\n",
    "        print(f\"The average {plotAttribute} for {topic} was {sum(yVals) / len(yVals)}\")\n",
    "    plt.plot(xVals, yVals, label=topic)\n",
    "    \n",
    "plt.title(f\"Daly Article output\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Number Of Articles\")\n",
    "# plt.legend()\n",
    "# plt.ylim((-1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot any attribute over time but broken down by outlet\n",
    "plotAttribute = \"publishCount\" # The article attribute to be avg'd and plotted over time (set to `publishCount` for daily TOTAL output)\n",
    "plotDates = getDateRange(startScrapeDate, endScrapeDate) # A list of all the dates that will be plotted\n",
    "\n",
    "displayOutlets = [] # The list that keeps track of which outlets to display\n",
    "if showOutletsList == []: # If the user has not specified which outlets to show, show all of them\n",
    "    for outlet in outletList:\n",
    "        displayOutlets.append(outlet.name) \n",
    "else:\n",
    "    displayOutlets = showOutletsList # Else show only those specified\n",
    "\n",
    "outletData = {}\n",
    "for topic in plotArticles.keys():\n",
    "    outletData[topic] = {}\n",
    "    for outlet in displayOutlets: # Show how each media outlet reports each topic\n",
    "        outletRow = [outlet]\n",
    "        plotData = {} # Dict containing each display date as key, and the list of scores for that day as value\n",
    "        for dateIndex, date in enumerate(plotDates):\n",
    "            if dateIndex + 1 != len(plotDates):\n",
    "                plotData[date] = []       \n",
    "        articleCount = 0\n",
    "        for article in plotArticles[topic]:\n",
    "            if article.outlet == outlet:\n",
    "                articleDate = article.date\n",
    "                try:\n",
    "                    if plotAttribute == \"publishCount\": # If the user is trying to find how many articles have been published on a given day, add 1 per article\n",
    "                        plotData[articleDate.replace(hour=0, minute=0, second=0)].append(1)\n",
    "                        articleCount += 1\n",
    "                    else:\n",
    "                        plotData[articleDate.replace(hour=0, minute=0, second=0)].append(getattr(article, plotAttribute)) # Append the score to the daily list\n",
    "                except KeyError:\n",
    "                    pass\n",
    "        outletData[topic][outlet] = articleCount\n",
    "        \n",
    "        # Plot the data\n",
    "        xVals = list(plotData.keys())\n",
    "        yVals = []\n",
    "        for val in xVals:\n",
    "            try:\n",
    "                if plotAttribute == \"publishCount\":\n",
    "                    yVals.append(len(plotData[val])) # Plot the daily count (total)\n",
    "                else:\n",
    "                    yVals.append(sum(plotData[val]) / len(plotData[val])) # plot the daily average \n",
    "            except ZeroDivisionError:\n",
    "                yVals.append(0) # If there are no datapoints for the day, display 0\n",
    "        plt.plot(xVals, yVals, label=f\"{outlet} - {topic}\")\n",
    "        \n",
    "        if plotAttribute == \"publishCount\":\n",
    "            print(f\"In total, {outlet} published {sum(yVals)} articles about {topic}\")\n",
    "        else:\n",
    "            print(f\"For {outlet} the overall average for {topic} was {round(sum(yVals) / len(yVals), 4)}\")\n",
    "\n",
    "\n",
    "            \n",
    "plt.title(f\"{plotAttribute} Over time by Outlet\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert outlet data to outputable csv data\n",
    "outputData = [['']+[topic for topic in plotArticles.keys()]]\n",
    "for outlet in outletData[list(plotArticles.keys())[0]].keys():\n",
    "    rowList = [outlet]\n",
    "    for topic in plotArticles.keys():\n",
    "        rowList.append(outletData[topic][outlet])\n",
    "    outputData.append(rowList)\n",
    "with open(\"./output.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as outputFile:\n",
    "    writer = csv.writer(outputFile)\n",
    "    writer.writerows(outputData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text outputs for each journalist by topic\n",
    "displayTopic = \"total\" # The topic that gets graphed\n",
    "journalistList = []\n",
    "journalistOutput = {}\n",
    "for article in plotArticles[displayTopic]:\n",
    "    for name in getNames(str(article.author), pos_tag, word_tokenize):\n",
    "        journalistList.append(name)\n",
    "        try:\n",
    "            if article.outlet not in journalistOutput[name]:\n",
    "               journalistOutput[name].append(article.outlet)\n",
    "\n",
    "        except KeyError:\n",
    "            journalistOutput[name] = [article.outlet]\n",
    "\n",
    "print(f\"The 10 most prolific journalists are:\")\n",
    "for journalist in Counter(journalistList).most_common(10):\n",
    "    print(f\"- {journalist[0]} - {journalist[1]} | {journalistOutput[journalist[0]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find keywords for each day by topic\n",
    "dailyDisplay = 4 # The number of keywords that gets displayed for each date\n",
    "displayTopic = \"total\" # The topic that gets graphed\n",
    "minTextScore = 2 # The minimum number of a times a keyword needs to be mentioned in order to get it's text displayed\n",
    "\n",
    "\n",
    "try:\n",
    "    plotArticles[displayTopic]\n",
    "except KeyError:\n",
    "    print(f\"Topic {displayTopic} is not available, the possible topics are {list(plotArticles.keys())}\")\n",
    "    \n",
    "\n",
    "keywordColors = {} # Dict containing the color for each keyword\n",
    "plotDates = getDateRange(startScrapeDate, endScrapeDate) # A list of all the dates that will be plotted\n",
    "totalKeywords = [] # All the keywords and their freqency\n",
    "\n",
    "datedKeywords = {} # A dict containg all the keywords in articles from a given date about the topic\n",
    "for dateIndex, date in enumerate(plotDates):\n",
    "    if dateIndex + 1 != len(plotDates):\n",
    "        datedKeywords[date] = []\n",
    "\n",
    "for article in plotArticles[displayTopic]:\n",
    "    if article.outlet in showOutletsList or showOutletsList == []: # If the article is from the specified outlet\n",
    "        articleDate = article.date\n",
    "        for word in article.headline.split(\" \"):\n",
    "            word = word.strip().lower()\n",
    "            if word not in stopwordsSet and len(word) > 2 and word not in exclusionList:\n",
    "                lemmatizedWord = lemmatizer.lemmatize(word)\n",
    "                totalKeywords.append(lemmatizedWord)\n",
    "                datedKeywords[articleDate.replace(hour=0, minute=0, second=0)].append(lemmatizedWord) # Append the (lemmatized) word to the dict for the given date\n",
    "lastKeywords = []\n",
    "for date in datedKeywords.keys():\n",
    "    keywords = Counter(datedKeywords[date]).most_common(dailyDisplay)\n",
    "    for keyword in keywords:\n",
    "        try:\n",
    "            keywordColor = keywordColors[keyword[0]] # If the keyword already has a color for itself\n",
    "        except KeyError:\n",
    "            r = lambda: random.randint(0,255) # Else, generate a new color for the keyword\n",
    "            keywordColor = '#%02X%02X%02X' % (r(),r(),r())\n",
    "            keywordColors[keyword[0]] = keywordColor # If this is the first time \n",
    "\n",
    "        plt.scatter(date, keyword[1], color=keywordColor, label=keyword[0]) # Put the point on the graph\n",
    "\n",
    "        # Draw lines between points with the same keyword\n",
    "        foundPrior = False # Tracks whether the date before contains the same keyword\n",
    "        for lastKeyword in lastKeywords:\n",
    "            if lastKeyword[0] == keyword[0]:\n",
    "                plt.plot([lastDate, date], [lastKeyword[1], keyword[1]], color=keywordColor)\n",
    "                foundPrior = True\n",
    "                break\n",
    "\n",
    "        if not foundPrior and keyword[1] >= minTextScore: # Only display text if the point is at the start of a 'chain'\n",
    "            plt.text(date, keyword[1], keyword[0])\n",
    "\n",
    "    # Save the last date and keywords to plot lines in the next date\n",
    "    lastDate = date \n",
    "    lastKeywords = keywords\n",
    "\n",
    "print(f\"The most common keywords for {displayTopic} were\")\n",
    "for keyword, keyFreq in Counter(totalKeywords).most_common(15):\n",
    "    print(f\"- {keyword} - {keyFreq}\")\n",
    "    \n",
    "plt.title(f\"Keywords over time for topic {displayTopic}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram for the number of articles with each sentiment scores\n",
    "displayTopic = \"france\" # The topic that gets graphed\n",
    "incrementCount = 50 # Number of bars to get shown\n",
    "\n",
    "try:\n",
    "    plotArticles[displayTopic]\n",
    "except KeyError:\n",
    "    print(f\"Topic {displayTopic} is not available, the possible topics are {list(plotArticles.keys())}\")\n",
    "    \n",
    "sentimentData = []\n",
    "\n",
    "outletData = {}\n",
    "for outlet in showOutletsList:\n",
    "    outletData[outlet] = {\"negative\": 0, \"neutral\": 0, \"positive\": 0}\n",
    "\n",
    "for article in plotArticles[displayTopic]:\n",
    "    sentimentData.append(article.sentimentScore)\n",
    "    if article.sentimentScore < -0.25:\n",
    "        outletData[article.outlet][\"negative\"] += 1\n",
    "    elif article.sentimentScore < 0.25:\n",
    "        outletData[article.outlet][\"neutral\"] += 1    \n",
    "    else:\n",
    "        outletData[article.outlet][\"positive\"] += 1\n",
    "    \n",
    "for outlet in showOutletsList:\n",
    "    print(f\"{outlet} - {outletData[outlet]['negative']} - {outletData[outlet]['neutral']} - {outletData[outlet]['positive']}\")\n",
    "    \n",
    "plt.hist(sentimentData, incrementCount, range=[-1,1])\n",
    "plt.title(f\"Number of articles with each sentiment for topic {displayTopic}\")\n",
    "plt.xlabel(\"Sentiment Value\")\n",
    "plt.ylabel(\"Number of Articles\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find the most common verb, adjective and so on (NOTE, does not exclude stopwords)\n",
    "displayTopic = \"france\" # The topic that gets graphed\n",
    "wordTypes = [\"JJ\", \"JJS\", \"JJR\"] # Word types (https://www.guru99.com/pos-tagging-chunking-nltk.html#:~:text=POS%20Tagging%20in%20NLTK%20is,each%20word%20of%20the%20sentence.)\n",
    "words = []\n",
    "tagList = []\n",
    "maxCount = -1 # The maximum amount of articles to be checked\n",
    "\n",
    "for articleIndex, article in enumerate(plotArticles[displayTopic]):\n",
    "    for word in article.headline.split(\" \"):\n",
    "        if(len(word) >= 2):\n",
    "            if(\"\" in wordTypes):\n",
    "                if word not in stopwordsSet:\n",
    "                    words.append(word)\n",
    "            else:\n",
    "                posTag = pos_tag([word])[0][1]\n",
    "                tagList.append(posTag)\n",
    "                if(posTag in wordTypes) or (\"\" in wordTypes):\n",
    "                    words.append(word)\n",
    "\n",
    "    print(f\"\\r Loading {articleIndex+1}/{len(plotArticles[displayTopic])}\", end=\"\")\n",
    "    if(articleIndex > maxCount) and (maxCount != -1):\n",
    "        break\n",
    "print(\"\")\n",
    "wordCount = Counter(words).most_common(10)\n",
    "tagCount = Counter(tagList).most_common(10)\n",
    "print(\"WORDS\")\n",
    "print(*[f\"-{word[0]} - {word[1]}\\n\" for word in wordCount])\n",
    "print(\"TAGS\")\n",
    "print(*[f\"{tag[0]}-{tag[1]}\\n\" for tag in tagCount])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABC News\n",
      "1A Government For  |1B Government Against  |2A Individual For  |2B Individual Against  |3A None For  |3B None Against  |4 NA  |\n",
      "AUKUS deal may be cold comfort to Aussies struggling with the cost of living. Can Labor make it about jobs?\n",
      "3a\n",
      "Ahead of AUKUS unveiling, Port Kembla emerges as preferred site for new submarine base\n",
      "3a\n",
      "Live: Australian nuclear submarine program to cost up to $368b as AUKUS details set to be unveiled\n",
      "1a\n",
      "Australian nuclear submarine program to cost up to $368b as AUKUS details unveiled\n",
      "1a\n",
      "No briefing required: China already knows AUKUS is about them. So what happens next?\n",
      "1b\n",
      "Nuclear submarines needed as region has become 'less stable', AUKUS task force head says\n",
      "1a\n",
      "The AUKUS deal is done but, in American politics, anything can be undone\n",
      "1a\n",
      "What these six words in Joe Biden's AUKUS speech told us about his next political battle\n",
      "2a\n",
      "Where will Australia dump its waste from the AUKUS nuclear submarines?\n",
      "3b\n",
      "Australia has to dispose of nuclear waste under the AUKUS submarines deal. Will this be in breach of a treaty?\n",
      "3b\n",
      "Fiji's PM tells Anthony Albanese he backs AUKUS deal\n",
      "1a\n",
      "Albanese secures Fiji's backing for AUKUS before he's even back in Australia\n",
      "1a\n",
      "Keating's AUKUS spray won't stop the subs deal but it may force the government to spell out the reason for it\n",
      "2b\n",
      "Premiers trade barbs across borders over who should host AUKUS nuclear waste\n",
      "1b\n",
      "US approves $1.3 billion sale of Tomahawk missiles to Australia under AUKUS pact\n",
      "4\n",
      "Australia's getting nuclear subs with the AUKUS deal,  but where will they be based?\n",
      "3a\n",
      "Support for potential Taiwan conflict not part of the AUKUS submarine deal, defence minister says\n",
      "1a\n",
      "Al Arabiya\n",
      "1A Government For  |1B Government Against  |2A Individual For  |2B Individual Against  |3A None For  |3B None Against  |4 NA  |\n",
      "China warns AUKUS alliance, says it’s on ‘path of error and danger’ with subs pact\n",
      "1b\n",
      "Alja Zeera\n",
      "1A Government For  |1B Government Against  |2A Individual For  |2B Individual Against  |3A None For  |3B None Against  |4 NA  |\n",
      "Australia to buy three nuclear subs under AUKUS pact: White House\n",
      "1a\n",
      "BBC News\n",
      "1A Government For  |1B Government Against  |2A Individual For  |2B Individual Against  |3A None For  |3B None Against  |4 NA  |\n",
      "US summit to finalise Aukus defence pact\n",
      "1a\n",
      "Aukus deal: US, UK and Australia agree on nuclear submarine project\n",
      "1a\n",
      "Aukus submarine deal: Is conflict with China getting closer?\n",
      "1b\n",
      "DW News\n",
      "1A Government For  |1B Government Against  |2A Individual For  |2B Individual Against  |3A None For  |3B None Against  |4 NA  |\n",
      "Islamic Republic News Agency\n",
      "1A Government For  |1B Government Against  |2A Individual For  |2B Individual Against  |3A None For  |3B None Against  |4 NA  |\n",
      "Russia Today\n",
      "1A Government For  |1B Government Against  |2A Individual For  |2B Individual Against  |3A None For  |3B None Against  |4 NA  |\n",
      "Details of AUKUS nuclear sub deal revealed\n",
      "1a\n",
      "China warns of ‘dangerous road’ following AUKUS deal\n",
      "1b\n",
      "AUKUS deal ‘worst in history’ – former Australian PM\n",
      "2b\n",
      "The AUKUS nuclear submarine deal is part of an imperialist crusade against China\n",
      "2b\n",
      "The AUKUS deal confirms Australia’s complete dependence on the US and the UK\n",
      "2b\n",
      "Pacific nation in talks to join AUKUS bloc\n",
      "1a\n"
     ]
    }
   ],
   "source": [
    "catergories = {\n",
    "    \"1a\": \"Government For\",\n",
    "    \"1b\": \"Government Against\",\n",
    "    \"2a\": \"Individual For\",\n",
    "    \"2b\": \"Individual Against\",\n",
    "    \"3a\": \"None For\",\n",
    "    \"3b\": \"None Against\",\n",
    "    \"4\": \"NA\"\n",
    "}\n",
    "\n",
    "catergoriesList = list(key.upper() + ' ' + catergories[key] + '  |' for key in catergories.keys())\n",
    "output = {\"count\": 0}\n",
    "\n",
    "for outlet in showOutletsList:\n",
    "    output[outlet] = {}\n",
    "    for key in catergories.keys():\n",
    "        output[outlet][catergories[key]] = 0\n",
    "    print(f\"{outlet}\\n{''.join(catergoriesList)}\")\n",
    "    for articleIndex, article in enumerate(plotArticles[\"aukus\"]):\n",
    "        if(article.outlet == outlet):\n",
    "            print(article.headline.strip())\n",
    "            sourceType = input()\n",
    "            try:\n",
    "                output[outlet][catergories[sourceType.lower()]] += 1\n",
    "            except KeyError:\n",
    "                print(\"WRONG\")\n",
    "                sourceType = input()\n",
    "                output[outlet][catergories[sourceType.lower()]] += 1\n",
    "            output[\"count\"] += 1\n",
    "            \n",
    "        with open(\"sources.json\", \"w\", encoding=\"utf-8\") as outputFile:\n",
    "            json_object = json.dumps(output, indent=4)\n",
    "            outputFile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
