{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a4e3e55",
   "metadata": {},
   "source": [
    "# Analyse server stored data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2734784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import time\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.style\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import operator\n",
    "\n",
    "from src.media import Outlet, Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e402d4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions\n",
    "def getDateRange(start_date, end_date):  # Return list of datetime.date objects between start_date and end_date (inclusive).\n",
    "    date_list = []\n",
    "    curr_date = start_date\n",
    "    while curr_date <= end_date:\n",
    "        date_list.append(curr_date)\n",
    "        curr_date += timedelta(days=1)\n",
    "    return date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6644714f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make maptlotlib show graphs in new window\n",
    "%matplotlib qt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2968257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "# Date range for articles being scraped from the server\n",
    "startScrapeDate = \"01/08/2022\"\n",
    "endScrapeDate = \"31/12/2022\"\n",
    "\n",
    "collectionCap = -1 # The maximum amount of articles to get pulled from the server (set to -1 for uncaped scraping)\n",
    "\n",
    "startScrapeDate = datetime.strptime(startScrapeDate, \"%d/%m/%Y\")\n",
    "endScrapeDate = datetime.strptime(endScrapeDate, \"%d/%m/%Y\")\n",
    "stopwordsSet = set(stopwords.words('english'))\n",
    "exclusionList = [\"say\", \"new\", \"news\", \"day\", \"days\"]\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c58a7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load setup data\n",
    "with open(\"./settings.json\", \"r\") as setupFile:\n",
    "    setupData = json.load(setupFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8591f4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Collected article number 171712\n",
      " Collected a total of 171712 articles in 51.999 seconds\n"
     ]
    }
   ],
   "source": [
    "articleList = []\n",
    "startScanTime = time.time() # Track the time elapsed \n",
    "DBClient = MongoClient(setupData[\"DB_URI\"], server_api = ServerApi('1')) # Connect to the database\n",
    "\n",
    "# Get articles from DBClient\n",
    "articleCollection = DBClient[setupData[\"DB_NAME\"]]['newsData']\n",
    "articleCursor = articleCollection.aggregate([{'$match': {'publishDate': {\n",
    "                '$gt': startScrapeDate, \n",
    "                '$lt': endScrapeDate\n",
    "        }}}])\n",
    "\n",
    "for articleIndex, article in enumerate(articleCursor):\n",
    "    articleList.append(Article(\n",
    "                article[\"outletName\"],\n",
    "                article[\"headline\"],\n",
    "                article[\"description\"],\n",
    "                article[\"author\"],\n",
    "                article[\"publishDate\"],\n",
    "                article[\"sentimentScore\"]\n",
    "        ))\n",
    "    \n",
    "    print(f\"\\r Collected article number {articleIndex + 1}\", end=\"\")\n",
    "print(f\"\\n Collected a total of {len(articleList)} articles in {round((time.time() - startScanTime), 3)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1deb0684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26 outlets in total\n"
     ]
    }
   ],
   "source": [
    "# Sort articles by outlet\n",
    "outletList = []\n",
    "\n",
    "for articleIndex, article in enumerate(articleList):\n",
    "    foundOutlet = False # If the outlet has been found within `outletList`\n",
    "    for outlet in outletList:\n",
    "        if outlet.name == article.outlet:\n",
    "            outlet.addArticle(article)\n",
    "            foundOutlet = True\n",
    "            break\n",
    "    if not foundOutlet: # Make new outlet\n",
    "        newOutlet = Outlet(article.outlet)\n",
    "        outletList.append(newOutlet)    \n",
    "print(f\"Found {len(outletList)} outlets in total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45e1f138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Daily Mail ===\n",
      "Published a total of 31734 articles\n",
      "Has an average sentiment of -0.206\n",
      "\n",
      "\n",
      "=== The Guardian ===\n",
      "Published a total of 18424 articles\n",
      "Has an average sentiment of -0.066\n",
      "\n",
      "\n",
      "=== CNN ===\n",
      "Published a total of 10848 articles\n",
      "Has an average sentiment of -0.092\n",
      "\n",
      "\n",
      "=== BuzzFeed ===\n",
      "Published a total of 10825 articles\n",
      "Has an average sentiment of 0.091\n",
      "\n",
      "\n",
      "=== South China Morning Post ===\n",
      "Published a total of 9894 articles\n",
      "Has an average sentiment of -0.092\n",
      "\n",
      "\n",
      "=== CBS News ===\n",
      "Published a total of 8561 articles\n",
      "Has an average sentiment of -0.054\n",
      "\n",
      "\n",
      "=== ABC News ===\n",
      "Published a total of 8488 articles\n",
      "Has an average sentiment of -0.112\n",
      "\n",
      "\n",
      "=== Huffington Post ===\n",
      "Published a total of 8007 articles\n",
      "Has an average sentiment of -0.067\n",
      "\n",
      "\n",
      "=== Sydney Morning Herald ===\n",
      "Published a total of 7537 articles\n",
      "Has an average sentiment of -0.073\n",
      "\n",
      "\n",
      "=== The Age ===\n",
      "Published a total of 7466 articles\n",
      "Has an average sentiment of -0.075\n",
      "\n",
      "\n",
      "=== Fox News ===\n",
      "Published a total of 7227 articles\n",
      "Has an average sentiment of -0.123\n",
      "\n",
      "\n",
      "=== Alja Zeera ===\n",
      "Published a total of 6395 articles\n",
      "Has an average sentiment of -0.128\n",
      "\n",
      "\n",
      "=== USA Today ===\n",
      "Published a total of 5451 articles\n",
      "Has an average sentiment of -0.066\n",
      "\n",
      "\n",
      "=== Amercian ABC ===\n",
      "Published a total of 4051 articles\n",
      "Has an average sentiment of -0.201\n",
      "\n",
      "\n",
      "=== BBC News ===\n",
      "Published a total of 3905 articles\n",
      "Has an average sentiment of -0.196\n",
      "\n",
      "\n",
      "=== New York Times ===\n",
      "Published a total of 3820 articles\n",
      "Has an average sentiment of -0.144\n",
      "\n",
      "\n",
      "=== 9 News ===\n",
      "Published a total of 3715 articles\n",
      "Has an average sentiment of -0.191\n",
      "\n",
      "\n",
      "=== CNBC ===\n",
      "Published a total of 2787 articles\n",
      "Has an average sentiment of -0.031\n",
      "\n",
      "\n",
      "=== Daily Telegraph ===\n",
      "Published a total of 2349 articles\n",
      "Has an average sentiment of -0.079\n",
      "\n",
      "\n",
      "=== Michael West ===\n",
      "Published a total of 1856 articles\n",
      "Has an average sentiment of -0.019\n",
      "\n",
      "\n",
      "=== The Conversation ===\n",
      "Published a total of 1821 articles\n",
      "Has an average sentiment of 0.006\n",
      "\n",
      "\n",
      "=== Sky News ===\n",
      "Published a total of 1820 articles\n",
      "Has an average sentiment of -0.232\n",
      "\n",
      "\n",
      "=== The Washington Post ===\n",
      "Published a total of 1773 articles\n",
      "Has an average sentiment of -0.209\n",
      "\n",
      "\n",
      "=== Crikey ===\n",
      "Published a total of 1326 articles\n",
      "Has an average sentiment of -0.064\n",
      "\n",
      "\n",
      "=== Wall Street Journal ===\n",
      "Published a total of 1141 articles\n",
      "Has an average sentiment of -0.149\n",
      "\n",
      "\n",
      "=== Independent Australia ===\n",
      "Published a total of 465 articles\n",
      "Has an average sentiment of -0.094\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outletList.sort(key = lambda x : len(x.articleList), reverse=True)\n",
    "# Text outputs for each outlet\n",
    "for outlet in outletList:\n",
    "    # Get the average sentiment\n",
    "    avgSentiment = sum(list(article.sentimentScore for article in outlet.articleList)) / len(outlet.articleList)\n",
    "    avgSentiment = avgSentiment \n",
    "        \n",
    "    print(f\"{'=' * 3} {outlet.name} {'=' * 3}\")\n",
    "    print(f\"Published a total of {len(outlet.articleList)} articles\")\n",
    "    print(f\"Has an average sentiment of {round(avgSentiment, 3)}\")\n",
    "    print(\"\\n\")\n",
    "#     print(f\"{outlet.name},{len(outlet.articleList)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b796a9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 most prolific journalists are:\n",
      "- None - 95199 | ['Daily Mail', 'Huffington Post', 'BuzzFeed', 'CNN', 'CBS News', 'Amercian ABC', 'The Conversation', 'Sky News', 'BBC News', 'Alja Zeera', 'CNBC', 'ABC News', 'Wall Street Journal', 'Fox News', 'New York Times']\n",
      "-  - 9699 | ['The Guardian', 'The Age', 'Sydney Morning Herald', 'Daily Telegraph', 'South China Morning Post', 'Independent Australia']\n",
      "- 9News - 3716 | ['9 News']\n",
      "- AAP - 1665 | ['Crikey', 'Michael West', 'The Guardian']\n",
      "- Associated Press - 1347 | ['South China Morning Post', 'The Guardian', 'USA Today', 'The Washington Post', 'Sydney Morning Herald', 'The Age']\n",
      "- Reuters - 1161 | ['South China Morning Post', 'Crikey', 'The Guardian', 'New York Times', 'The Age', 'Sydney Morning Herald']\n",
      "- Agence France-Presse - 977 | ['South China Morning Post', 'The Guardian']\n",
      "- Australian Associated Press - 660 | ['The Guardian']\n",
      "- Bloomberg - 468 | ['South China Morning Post']\n",
      "- Fox News Staff - 375 | ['Fox News']\n"
     ]
    }
   ],
   "source": [
    "# Text outputs for each journalist\n",
    "journalistList = []\n",
    "journalistOutput = {}\n",
    "for article in articleList:\n",
    "    journalistList.append(article.author)\n",
    "    try:\n",
    "        if article.outlet not in journalistOutput[article.author]:\n",
    "           journalistOutput[article.author].append(article.outlet)\n",
    "        \n",
    "    except KeyError:\n",
    "        journalistOutput[article.author] = [article.outlet]\n",
    "\n",
    "print(f\"The 10 most prolific journalists are:\")\n",
    "for journalist in Counter(journalistList).most_common(10):\n",
    "    print(f\"- {journalist[0]} - {journalist[1]} | {journalistOutput[journalist[0]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a25483a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display parameters\n",
    "topicList = [[\"mar-a-lago\",\"trump\",\"donald\",\"fbi\",\"raid\",\"seize\",\"documents\"]] # Which topics to use (leave blank for all) (MUST BE LOWERCASE)\n",
    "showOutletsList = [] # Which outlets to be shown (leave blank for all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2e9d031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect data to plot\n",
    "plotArticles = {} # Stores the articles by the topic \n",
    "if topicList == []:\n",
    "    plotArticles['total'] = [] # If there are no topic, total is used to show the total amt of articles\n",
    "    for article in articleList:\n",
    "        if article.outlet in showOutletsList or showOutletsList == []:\n",
    "            plotArticles['total'].append(article)\n",
    "else:            \n",
    "    for topic in topicList: \n",
    "        plotArticles[topic[0]] = [] # Each topic stores corresponding articles in a list\n",
    "        for article in articleList:\n",
    "            if article.outlet in showOutletsList or showOutletsList == []: # If the article is from the specified outlet\n",
    "                for word in article.headline.split(\" \"):\n",
    "                    if word.lower() in topic: # If a given word from the article is in the topic searchlist\n",
    "                        plotArticles[topic[0]].append(article)\n",
    "                        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2daff92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total, 4518 articles got published about mar-a-lago\n"
     ]
    }
   ],
   "source": [
    "# Plot daily average (or total output) for any attribute over time, as a total/avg of all outlets\n",
    "plotAttribute = \"publishCount\" # The article attribute to be avg'd and plotted over time (set to `publishCount` for daily TOTAL output)\n",
    "plotDates = getDateRange(startScrapeDate, endScrapeDate)\n",
    "\n",
    "for topic in plotArticles.keys():\n",
    "    plotData = {}\n",
    "    for dateIndex, date in enumerate(plotDates):\n",
    "        if dateIndex + 1 != len(plotDates):\n",
    "            plotData[date] = []\n",
    "    for article in plotArticles[topic]:\n",
    "        articleDate = article.date\n",
    "        if plotAttribute == \"publishCount\":\n",
    "            plotData[articleDate.replace(hour=0, minute=0, second=0)].append(1)\n",
    "        else:\n",
    "            plotData[articleDate.replace(hour=0, minute=0, second=0)].append(getattr(article, plotAttribute))\n",
    "    \n",
    "    # Plot the data\n",
    "    xVals = list(plotData.keys())\n",
    "    yVals = []\n",
    "    for val in xVals:\n",
    "        try:\n",
    "            if plotAttribute == \"publishCount\":\n",
    "                yVals.append(len(plotData[val]))\n",
    "            else:\n",
    "                yVals.append(sum(plotData[val]) / len(plotData[val]))\n",
    "        except ZeroDivisionError:\n",
    "            yVals.append(0)\n",
    "    if plotAttribute == \"publishCount\":\n",
    "        print(f\"In total, {sum(yVals)} articles got published about {topic}\")\n",
    "    else:\n",
    "        print(f\"The average {plotAttribute} for {topic} was {sum(yVals) / len(yVals)}\")\n",
    "    plt.plot(xVals, yVals, label=topic)\n",
    "    \n",
    "plt.title(f\"{plotAttribute} over time\")\n",
    "plt.legend()\n",
    "# plt.ylim((-1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ee6dc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily Mail,37\n",
      "The Guardian,17\n",
      "BuzzFeed,2\n",
      "CNN,30\n",
      "South China Morning Post,3\n",
      "ABC News,1\n",
      "Sydney Morning Herald,4\n",
      "The Age,4\n",
      "CBS News,10\n",
      "Huffington Post,14\n",
      "Fox News,20\n",
      "Alja Zeera,3\n",
      "USA Today,17\n",
      "BBC News,4\n",
      "New York Times,1\n",
      "9 News,0\n",
      "Amercian ABC,4\n",
      "Daily Telegraph,0\n",
      "CNBC,4\n",
      "Crikey,0\n",
      "The Conversation,2\n",
      "Sky News,2\n",
      "Michael West,0\n",
      "Wall Street Journal,0\n",
      "Independent Australia,0\n",
      "Daily Mail,2\n",
      "The Guardian,0\n",
      "BuzzFeed,0\n",
      "CNN,14\n",
      "South China Morning Post,0\n",
      "ABC News,17\n",
      "Sydney Morning Herald,0\n",
      "The Age,0\n",
      "CBS News,0\n",
      "Huffington Post,152\n",
      "Fox News,0\n",
      "Alja Zeera,0\n",
      "USA Today,0\n",
      "BBC News,1\n",
      "New York Times,1\n",
      "9 News,2\n",
      "Amercian ABC,69\n",
      "Daily Telegraph,0\n",
      "CNBC,6\n",
      "Crikey,0\n",
      "The Conversation,0\n",
      "Sky News,2\n",
      "Michael West,0\n",
      "Wall Street Journal,0\n",
      "Independent Australia,8\n"
     ]
    }
   ],
   "source": [
    "# Plot any attribute over time but broken down by outlet\n",
    "plotAttribute = \"publishCount\" # The article attribute to be avg'd and plotted over time (set to `publishCount` for daily TOTAL output)\n",
    "plotDates = getDateRange(startScrapeDate, endScrapeDate) # A list of all the dates that will be plotted\n",
    "\n",
    "displayOutlets = [] # The list that keeps track of which outlets to display\n",
    "if showOutletsList == []: # If the user has not specified which outlets to show, show all of them\n",
    "    for outlet in outletList:\n",
    "        displayOutlets.append(outlet.name) \n",
    "else:\n",
    "    displayOutlets = showOutletsList # Else show only those specified\n",
    "\n",
    "for topic in plotArticles.keys():\n",
    "    for outlet in displayOutlets: # Show how each media outlet reports each topic\n",
    "        plotData = {} # Dict containing each display date as key, and the list of scores for that day as value\n",
    "        for dateIndex, date in enumerate(plotDates):\n",
    "            if dateIndex + 1 != len(plotDates):\n",
    "                plotData[date] = []\n",
    "                \n",
    "        for article in plotArticles[topic]:\n",
    "            if article.outlet == outlet:\n",
    "                articleDate = article.date\n",
    "                if plotAttribute == \"publishCount\": # If the user is trying to find how many articles have been published on a given day, add 1 per article\n",
    "                    plotData[articleDate.replace(hour=0, minute=0, second=0)].append(1)\n",
    "                else:\n",
    "                    plotData[articleDate.replace(hour=0, minute=0, second=0)].append(getattr(article, plotAttribute)) # Append the score to the daily list\n",
    "        \n",
    "        # Plot the data\n",
    "        xVals = list(plotData.keys())\n",
    "        yVals = []\n",
    "        for val in xVals:\n",
    "            try:\n",
    "                if plotAttribute == \"publishCount\":\n",
    "                    yVals.append(len(plotData[val])) # Plot the daily count (total)\n",
    "                else:\n",
    "                    yVals.append(sum(plotData[val]) / len(plotData[val])) # plot the daily average \n",
    "            except ZeroDivisionError:\n",
    "                yVals.append(0) # If there are no datapoints for the day, display 0\n",
    "        plt.plot(xVals, yVals, label=f\"{outlet} - {topic}\")\n",
    "        \n",
    "        if plotAttribute == \"publishCount\":\n",
    "#             print(f\"In total, {outlet} published {sum(yVals)} articles about {topic}\")\n",
    "            print(f\"{outlet},{sum(yVals)}\")\n",
    "        else:\n",
    "            print(f\"For {outlet} the overall average for {topic} was {round(sum(yVals) / len(yVals), 4)}\")\n",
    "plt.title(f\"{plotAttribute} Over time by Outlet\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd2cbb24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common keywords for mar-a-lago were\n",
      "- trump - 152\n",
      "- mar-a-lago - 41\n",
      "- fbi - 37\n",
      "- donald - 36\n",
      "- document - 22\n",
      "- say - 20\n",
      "- 2024 - 10\n",
      "- raid - 10\n",
      "- search - 9\n",
      "- special - 9\n",
      "- tax - 9\n",
      "- gop - 7\n",
      "- judge - 7\n",
      "- committee - 7\n",
      "- court - 6\n"
     ]
    }
   ],
   "source": [
    "# Find keywords for each day by topic\n",
    "dailyDisplay = 4 # The number of keywords that gets displayed for each date\n",
    "displayTopic = \"mar-a-lago\" # The topic that gets graphed\n",
    "minTextScore = 2 # The minimum number of a times a keyword needs to be mentioned in order to get it's text displayed\n",
    "\n",
    "\n",
    "try:\n",
    "    plotArticles[displayTopic]\n",
    "except KeyError:\n",
    "    print(f\"Topic {displayTopic} is not available, the possible topics are {list(plotArticles.keys())}\")\n",
    "    \n",
    "\n",
    "keywordColors = {} # Dict containing the color for each keyword\n",
    "plotDates = getDateRange(startScrapeDate, endScrapeDate) # A list of all the dates that will be plotted\n",
    "totalKeywords = [] # All the keywords and their freqency\n",
    "\n",
    "datedKeywords = {} # A dict containg all the keywords in articles from a given date about the topic\n",
    "for dateIndex, date in enumerate(plotDates):\n",
    "    if dateIndex + 1 != len(plotDates):\n",
    "        datedKeywords[date] = []\n",
    "\n",
    "for article in plotArticles[displayTopic]:\n",
    "    if article.outlet in showOutletsList or showOutletsList == []: # If the article is from the specified outlet\n",
    "        articleDate = article.date\n",
    "        for word in article.headline.split(\" \"):\n",
    "            word = word.strip().lower()\n",
    "            if word not in stopwordsSet and len(word) > 2 and word not in exclusionList:\n",
    "                datedKeywords[articleDate.replace(hour=0, minute=0, second=0)].append(lemmatizer.lemmatize(word)) # Append the (lemmatized) word to the dict for the given date\n",
    "lastKeywords = []\n",
    "for date in datedKeywords.keys():\n",
    "    keywords = Counter(datedKeywords[date]).most_common(dailyDisplay)\n",
    "    for keyword in keywords:\n",
    "        try:\n",
    "            keywordColor = keywordColors[keyword[0]] # If the keyword already has a color for itself\n",
    "        except KeyError:\n",
    "            r = lambda: random.randint(0,255) # Else, generate a new color for the keyword\n",
    "            keywordColor = '#%02X%02X%02X' % (r(),r(),r())\n",
    "            keywordColors[keyword[0]] = keywordColor # If this is the first time \n",
    "\n",
    "        totalKeywords.append(keyword[0])\n",
    "        plt.scatter(date, keyword[1], color=keywordColor, label=keyword[0]) # Put the point on the graph\n",
    "\n",
    "        # Draw lines between points with the same keyword\n",
    "        foundPrior = False # Tracks whether the date before contains the same keyword\n",
    "        for lastKeyword in lastKeywords:\n",
    "            if lastKeyword[0] == keyword[0]:\n",
    "                plt.plot([lastDate, date], [lastKeyword[1], keyword[1]], color=keywordColor)\n",
    "                foundPrior = True\n",
    "                break\n",
    "\n",
    "        if not foundPrior and keyword[1] >= minTextScore: # Only display text if the point is at the start of a 'chain'\n",
    "            plt.text(date, keyword[1], keyword[0])\n",
    "\n",
    "    # Save the last date and keywords to plot lines in the next date\n",
    "    lastDate = date \n",
    "    lastKeywords = keywords\n",
    "\n",
    "print(f\"The most common keywords for {displayTopic} were\")\n",
    "for keyword, keyFreq in Counter(totalKeywords).most_common(15):\n",
    "    print(f\"- {keyword} - {keyFreq}\")\n",
    "    \n",
    "plt.title(f\"Keywords over time for topic {displayTopic}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28e06e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram for the number of articles with each sentiment scores\n",
    "displayTopic = \"mar-a-lago\" # The topic that gets graphed\n",
    "incrementCount = 50\n",
    "counts = [0, 0]\n",
    "\n",
    "try:\n",
    "    plotArticles[displayTopic]\n",
    "except KeyError:\n",
    "    print(f\"Topic {displayTopic} is not available, the possible topics are {list(plotArticles.keys())}\")\n",
    "    \n",
    "sentimentData = []\n",
    "for article in plotArticles[displayTopic]:\n",
    "    if(article.sentimentScore > 0):\n",
    "        counts[1] += 1\n",
    "    elif(article.sentimentScore < 0):\n",
    "        counts[0] += 1\n",
    "    sentimentData.append(article.sentimentScore)\n",
    "    \n",
    "plt.hist(sentimentData, incrementCount)\n",
    "plt.title(f\"Number of articles with each sentiment for topic {displayTopic}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a9fb3a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump is accused of using copyrighted images in his NFT collection\n",
      "Trump Still Acting Like Imperious President In Mar-A-Lago: Report\n",
      "Emails show the FBI 'repeatedly grilled' Twitter execs over 'state propaganda' on the app\n",
      "Hong Kong police arrest 3, seize HK$700,000 worth of drugs after boy, 14, found dead in Chungking Mansions\n",
      "Three Jordanian police killed during raid on hideout in Maan\n",
      "Youth gang accused granted bail after city hotel raid\n",
      "Pakistani Taliban overpower guards, seize police center\n",
      "FBI warns of explosion of ‘sextortion’ cases targeting boys, teens\n",
      "Canada moves to seize US$26 million from Russian oligarch Roman Abramovich\n",
      "FBI issues alert on 'explosion' in child 'sextortion' schemes\n",
      "Jan. 6 Committee Refers Four Criminal Charges Against Trump to DOJ\n",
      "Trump should face criminal charges over Capitol riots, committee recommends\n",
      "January 6 committee recommends insurrection, obstruction charges against Donald Trump\n",
      "Pentagon Officials Feared Trump Would Try To Use Troops In His Jan. 6 Coup Attempt\n",
      "Rep. Liz Cheney, in opening statement, says Trump \"is unfit for any office\"\n",
      "US Capitol riot: Lawmakers recommend filing charges against Trump\n",
      "Jan. 6 Committee Says Donald Trump Tried To Bribe Witnesses\n",
      "Jan. 6 Committee Says Donald Trump Associates Tried To Bribe Witnesses\n",
      "Donald Trump referred for criminal prosecution over US Capitol attack\n",
      "Donald Trump referred for criminal prosecution over US Capitol attack\n",
      "Ohio CBP officers seize nearly 30,000 pills, 700 jelly packs of Sildenafil, active Viagra ingredient\n",
      "Jan. 6 committee condemns Trump as 'central cause' of insurrection in sweeping report\n",
      "Jan. 6 committee approves criminal referrals targeting Trump\n",
      "McCabe on how the DOJ is receiving Trump criminal referral\n",
      "Donald Trump should face criminal charges, January 6 riot panel says\n",
      "Donald Trump should face criminal charges, US Capitol riot panel says\n",
      "January 6 panel urges criminal charges against Donald Trump\n",
      "Madalina Cojocari: FBI expanding search for missing 11-year-old girl in North Carolina\n",
      "Hicks describes conversation with Trump in newly released video\n",
      "January 6 panel slams Ivanka Trump for not being 'forthcoming' in her testimony\n",
      "January 6 committee approves criminal referrals against Donald Trump\n",
      "January 6 committee approves criminal referrals against Donald Trump\n",
      "House Jan. 6 panel recommends DOJ prosecute Trump on several charges. Which ones and why?\n",
      "Opinion: Criminal referrals against Trump are a necessary step\n",
      "House Jan. 6 committee announces it recommends criminal charges against Trump\n",
      "Jan 6 committee refers Donald Trump for criminal prosecution on four counts – live\n",
      "Hope Hicks Reveals Ominous Trump Claim: ‘The Only Thing That Matters Is Winning’\n",
      "FBI official turned Twitter counsel Jim Baker thanked Bureau for suppressing Hunter Biden laptop\n",
      "January 6 committee recommends CRIMINAL charges for Trump over Capitol riot\n",
      "House Jan. 6 committee votes to refer criminal charges for Trump\n",
      "Jan. 6 committee sends DOJ historic criminal referral of Trump over Capitol riot\n",
      "House January 6 panel recommends criminal charges against Donald Trump – video\n",
      "Your Tuesday Briefing: Jan. 6 Panel Refers Trump for Charges\n",
      "Special Report: House Jan. 6 committee recommends criminal charges for Trump in final meeting\n",
      "Jan. 6 committee vs. Donald Trump: A tale of the most obvious charges ever alleged\n",
      "Panel calls for Trump to face insurrection charge\n",
      "Mulvaney: This criminal referral should frighten Trump most\n",
      "Explaining the criminal charges January 6 committee recommended for Trump\n",
      "Inciting insurrection: a striking condemnation of Trump - but a high bar for prosecutors\n",
      "Opinion: Trump represents our founders' worst fears\n"
     ]
    }
   ],
   "source": [
    "for article in plotArticles[displayTopic]:\n",
    "    articleDate = article.date\n",
    "    articleDate = articleDate.replace(hour=0, minute=0, second=0)\n",
    "#     print(str(articleDate))\n",
    "    if (str(articleDate)) == \"2022-12-19 00:00:00\":\n",
    "        print(article.headline.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0147d681",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
